{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54993b70-da49-4ad2-8883-9052065af4d2",
      "metadata": {
        "id": "54993b70-da49-4ad2-8883-9052065af4d2"
      },
      "source": [
        "# Klasifikasi Citra Sederhana"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65cd3b28-cd57-47a4-ac8a-1fde973ae7aa",
      "metadata": {
        "id": "65cd3b28-cd57-47a4-ac8a-1fde973ae7aa"
      },
      "source": [
        "## 1. Mengimport Modul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fcc206d1-1bdb-4674-a132-35cd6840ba3b",
      "metadata": {
        "id": "fcc206d1-1bdb-4674-a132-35cd6840ba3b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684306bb-7792-4816-a6df-26a0e6c05795",
      "metadata": {
        "id": "684306bb-7792-4816-a6df-26a0e6c05795"
      },
      "source": [
        "### 2. Menginput Gambar dan Membuatnya dalam bentuk Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c5435ee7-2f21-4664-8ce9-8fe34e8bebc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "c5435ee7-2f21-4664-8ce9-8fe34e8bebc0",
        "outputId": "9ed17773-bc25-4931-c86e-d790a2aca8c0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Direktori /content/C:/Users/umalb/Dataset/dataset_kaggle_dedeikhsan/RiceLeafsDisease tidak ditemukan",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6bddd91736d2>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Mengecek apakah direktori tersebut jalan atau tidak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsolute_image_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Direktori {absolute_image_dir} tidak ditemukan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Direktori /content/C:/Users/umalb/Dataset/dataset_kaggle_dedeikhsan/RiceLeafsDisease tidak ditemukan"
          ]
        }
      ],
      "source": [
        "# Contoh Penggunaan\n",
        "image_dir = 'C:/Users/umalb/Dataset/dataset_kaggle_dedeikhsan/RiceLeafsDisease' #address folder gambar untuk dataset\n",
        "original_image_size = (1600,1600) #input ukuran pada gambar asli\n",
        "target_image_size = (224,224) # ukuran gambar yang diinginkan setelah diresize\n",
        "batch_size = 32 # ukuran batch awal sebesar 32\n",
        "\n",
        "# Mendapatkan path absolut dari direktori\n",
        "current_directory = os.getcwd()\n",
        "absolute_image_dir = os.path.join(current_directory, image_dir)\n",
        "\n",
        "#Mengecek apakah direktori tersebut jalan atau tidak\n",
        "if not os.path.exists(absolute_image_dir):\n",
        "  raise FileNotFoundError(f\"Direktori {absolute_image_dir} tidak ditemukan\")\n",
        "\n",
        "\n",
        "# Membuat Dataset Latihan (80% dari data total)\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory = absolute_image_dir, #variabel pengarahan address dataset\n",
        "    validation_split = 0.2, #20% data ini digunakan untuk validasi\n",
        "    subset = 'training',\n",
        "    seed = 123, #Random Seed untuk pergantian (shuffling)\n",
        "    image_size = original_image_size, # untuk menspesifikasi ukuran gambar pd variabel image_size\n",
        "    batch_size = batch_size)\n",
        "\n",
        "# Membuat Dataset Validasi (20% dari data total)\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory = absolute_image_dir,\n",
        "    validation_split = 0.2, # 20% data tersebut untuk validasi\n",
        "    subset = 'validation',\n",
        "    seed = 123, # Random Seed untuk Pergantian (Data acak)\n",
        "    image_size = original_image_size,\n",
        "    batch_size = batch_size) # sebagai variabel input untuk mendapatkan nilai akurasi, loss, dll pada validasi\n",
        "\n",
        "# Fungsi untuk meresize gambar\n",
        "def resize_images(images, size):\n",
        "  return tf.image.resize(images,size)\n",
        "\n",
        "# Resize gambar - gambar dalam dataset latihan\n",
        "train_dataset = train_dataset.map(lambda x, y: (resize_images(x, target_image_size), y))\n",
        "\n",
        "# Resize gambar-gambar dalam dataset validasi\n",
        "val_dataset = val_dataset.map(lambda x, y: (resize_images(x, target_image_size), y))\n",
        "\n",
        "# print Nama Kelas\n",
        "class_names = train_dataset.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc75b87-4234-4971-aa90-37b0cc92a06d",
      "metadata": {
        "id": "9dc75b87-4234-4971-aa90-37b0cc92a06d"
      },
      "source": [
        "### 3. Menampilkan Dataset awal dari Sampel Acak sebanyak 20 Gambar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63939d2a-4939-40e6-b180-7a2a8e00f8cb",
      "metadata": {
        "id": "63939d2a-4939-40e6-b180-7a2a8e00f8cb"
      },
      "outputs": [],
      "source": [
        "# Menampilkan 4x4 grid pada Gambar Latihan dari Dataset Tensorflow menggunakan tf.keras.utils.image_dataset_from_directory\n",
        "figur = plt.figure(figsize = (13,13))\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range (20): #menampilkan 20 sampel random gambar latihan\n",
        "        ax = plt.subplot(5,4, i+1)\n",
        "        plt.imshow(images[i].numpy().astype('uint8'))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbee31fe-d61e-48b4-9654-fc54c7686235",
      "metadata": {
        "id": "dbee31fe-d61e-48b4-9654-fc54c7686235"
      },
      "outputs": [],
      "source": [
        "# Menampilkan 4x4 grid pada Gambar Validasi dari Dataset Tensorflow menggunakan tf.keras.utils.image_dataset_from_directory\n",
        "figur = plt.figure(figsize = (13,13))\n",
        "for images, labels in val_dataset.take(1):\n",
        "    for i in range (20): #menampilkan 20 sampel random gambar latihan\n",
        "        ax = plt.subplot(5,4, i+1)\n",
        "        plt.imshow(images[i].numpy().astype('uint8'))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539fe74d-eba5-474f-a197-58e8c5a4912a",
      "metadata": {
        "id": "539fe74d-eba5-474f-a197-58e8c5a4912a"
      },
      "source": [
        "## 4. Melakukan Ekstraksi Fitur Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49d03c8-ae66-48a5-be8c-bd57ad2c9493",
      "metadata": {
        "id": "b49d03c8-ae66-48a5-be8c-bd57ad2c9493"
      },
      "outputs": [],
      "source": [
        "# Langkah 1 : Membuat Dataset Tensorflow dari Images Directory\n",
        "def create_dataset(image_dir, image_size, batch_size):\n",
        "    #Memuat dan memproses gambar dari Directory tersebut\n",
        "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "    # Menggunakan ImageDataGenerator untuk memuat dan memproses gambar dari direktori.\n",
        "    # Rescale 1./255 digunakan untuk mengubah rentang piksel menjadi antara 0 dan 1.\n",
        "    dataset = datagen.flow_from_directory(\n",
        "        directory=image_dir,\n",
        "        target_size = image_size, # ukuran target gambar yang diinginkan\n",
        "        batch_size = batch_size, # ukuran batch data yang akan dimuat pada setiap iterasi\n",
        "        class_mode = None, # Mode kelas. None berarti kita tidak membutuhkan label kelas karena kita menggunakan dataset ini untuk prediksi, bukan klasifikasi.\n",
        "        shuffle = False # shuffle=False berarti kita ingin menjaga urutan gambar sesuai dengan yang ada di direktori.\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Langkah 2 : mendefinisikan AlexNet Model untuk Ekstraksi Fitur\n",
        "def create_alexnet_model(input_shape, num_classes):\n",
        "    # Buat Input Layer\n",
        "    input_layer = Input(shape = input_shape)\n",
        "    #Conv Layer 1\n",
        "    x = Conv2D(filters=96, kernel_size=(11,11), strides = (4,4), activation = 'relu')(input_layer)\n",
        "    x = MaxPooling2D(pool_size = (3,3), strides = (2,2))(x)\n",
        "    #Conv Layer 2\n",
        "    x = Conv2D(filters=256, kernel_size=(5,5), padding='same', activation = 'relu')\n",
        "    x = MaxPooling2D(pool_size = (3,3), strides = (2,2))(x)\n",
        "    #Conv Layer 3\n",
        "    x = Conv2D(filters=384, kernel_size=(3,3), padding='same', activation = 'relu')\n",
        "    #Conv Layer 4\n",
        "    x = Conv2D(filters=384, kernel_size=(3,3), padding='same', activation = 'relu')\n",
        "    #Conv Layer 5\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation = 'relu')\n",
        "    x = MaxPooling2D(pool_size = (3,3), strides = (2,2))(x)\n",
        "    #Flatten Layer\n",
        "    x = Flatten()(x)\n",
        "    #Fully Connected Layer 1\n",
        "    x = Dense(4096, activation = 'relu')(x)\n",
        "    #Fully Connected Layer 2\n",
        "    x = Dense(4096, activation = 'relu')(x)\n",
        "    #Output Layer\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "    #Buat Model dari input layer dan output layer\n",
        "    model = tf.keras.Model(inputs=input_layer, output=output_layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Langkah 3 : Mendeteksi Penyakit Daun Padi dan Menyediakan Bounding Boxes\n",
        "def detect_and_bbox(dataset, model):\n",
        "    predictions = model.predict(dataset, verbose =1)\n",
        "    return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299b5e16-e294-4142-860b-20143ce2bc28",
      "metadata": {
        "id": "299b5e16-e294-4142-860b-20143ce2bc28"
      },
      "source": [
        "### 5. Mendefinisikan AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768c15bb-c604-41a5-8fc0-cd7935a050a7",
      "metadata": {
        "id": "768c15bb-c604-41a5-8fc0-cd7935a050a7",
        "outputId": "e80c08cf-a03c-4def-ba58-c9c4f080744e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'input_shape' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_shape(\u001b[38;5;241m*\u001b[39mimage_size, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m#bentuk daripada input gambar\u001b[39;00m\n\u001b[0;32m      2\u001b[0m alexnet_model \u001b[38;5;241m=\u001b[39m create_alexnet_model(input_shape)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'input_shape' is not defined"
          ]
        }
      ],
      "source": [
        "input_shape(*image_size, 3) #bentuk daripada input gambar\n",
        "num_classes = class_name # menentukan kelas dataset\n",
        "# Buat model AlexNet\n",
        "alexnet_model = create_alexnet_model(input_shape, num_classes)\n",
        "# menampilkan ringkasan model\n",
        "alexnet_model.summary() #digunakan untuk memeriksa arsitektur yang kita buat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4569b275-5f89-4387-9e7d-e83c332c526f",
      "metadata": {
        "id": "4569b275-5f89-4387-9e7d-e83c332c526f"
      },
      "source": [
        "### 6. Mengcompile AlexNet dengan Pendekatan Aturan untuk TL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e5baa48-bde2-4cbe-85fd-dc3565d4782f",
      "metadata": {
        "id": "6e5baa48-bde2-4cbe-85fd-dc3565d4782f"
      },
      "outputs": [],
      "source": [
        "# Menyusun Model AlexNet dengan Pendekatan untuk Transfer Learning\n",
        "alexnet_model.compile(optimizer = 'Adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                     metrics = ['accuracy']) # Loss = 'mean_squared_error', metrics = ['mse']\n",
        "\n",
        "num_epochs = 20\n",
        "# Melatih model tersebut dan mendapatkan jejak latihan\n",
        "# alexnet_model.fit(train_dataset, epoch=num_epochs, validation_data = val_dataset)\n",
        "history = alexnet_model.fit(train_dataset, epoch=num_epochs, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "960a64fb-32f7-43f1-86e7-bcef2c506d63",
      "metadata": {
        "id": "960a64fb-32f7-43f1-86e7-bcef2c506d63"
      },
      "source": [
        "## 7. Membuat Plot untuk Loss pada Latihan dan Validasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20639535-c192-4edc-bf36-61d6fb541493",
      "metadata": {
        "id": "20639535-c192-4edc-bf36-61d6fb541493"
      },
      "outputs": [],
      "source": [
        "# Membuat Plot Latihan Vs Plot Validasi\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Training Data Accuracy')\n",
        "plt.plot(history.history['val_loss'], label = 'Validation Data Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b85ed5-a11b-48fc-a99a-88a7ab1bb32e",
      "metadata": {
        "id": "c0b85ed5-a11b-48fc-a99a-88a7ab1bb32e"
      },
      "outputs": [],
      "source": [
        "# Menampilkan nilai akurasi dan loss pada nilai set validasi\n",
        "loss, accuracy = alexnet_model.evaluate(val_dataset)\n",
        "print(f\"Validation Loss {loss}\")\n",
        "print(f\"Validation Accuracy {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f8bec5-042f-4bb0-9866-c8c92f16de22",
      "metadata": {
        "id": "a7f8bec5-042f-4bb0-9866-c8c92f16de22"
      },
      "source": [
        "### 8. Mendeteksi Penyakit Daun Padi dan Menyediakan Kotak Pembatas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d78f1c9-0f79-4fc1-8226-397448fb392c",
      "metadata": {
        "id": "8d78f1c9-0f79-4fc1-8226-397448fb392c",
        "outputId": "57e4fef5-a780-4c22-d3db-c559bc4123a8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'detect_and_bbox' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Mendeteksi dan membuat Kotak Pembatas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m detect_and_bbox(val_dataset, alexnet_model)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'detect_and_bbox' is not defined"
          ]
        }
      ],
      "source": [
        "# Mendeteksi dan membuat Kotak Pembatas\n",
        "predictions = detect_and_bbox(val_dataset, alexnet_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7719aefe-4ecd-4928-9edd-39c14a097798",
      "metadata": {
        "id": "7719aefe-4ecd-4928-9edd-39c14a097798"
      },
      "source": [
        "### 9. Menampilkan Konfusi Matriks Data Latihan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf332984-513f-4634-8a8a-69fcb804e39b",
      "metadata": {
        "id": "bf332984-513f-4634-8a8a-69fcb804e39b"
      },
      "outputs": [],
      "source": [
        "# Menampilkan Confusi Matriks dalam Data Latihan\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Contoh Penggunaan\n",
        "num_runs = 5 # Bilangan running rerata sampai lebih dari -n\n",
        "dataset = train_dataset # dataset tensorflow\n",
        "# model = tf.keras.models.load_model('model_class_13_balances_combined_ep_50.h5')  # Your trained TensorFlow model\n",
        "\n",
        "def compute_confusion_matrix(alexnet_model, dataset, num_runs):\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "\n",
        "        # Membuat Prediksi dalam dataset\n",
        "        for features, labels in dataset:\n",
        "            predictions = alexnet_model.predict(features)\n",
        "            true_labels.extend(labels.numpy())\n",
        "            pred_labels.extend(np.argmax(predictions, axis = 1))\n",
        "\n",
        "        all_true_labels.append(true_labels)\n",
        "        all_pred_labels.append(pred_labels)\n",
        "\n",
        "    #Mengkomputasi Confusi Matrix untuk tiap Running\n",
        "    all_conf_matrices = [confusion_matrix(true_labels, pred_labels) for true_labels, pred_labels in zip(all_true_labels, all_pred_labels)]\n",
        "\n",
        "    # rerata Confusi Matrix\n",
        "    avg_conf_matrix = np.mean(all_conf_matrices, axis=0)\n",
        "\n",
        "    return avg_conf_matrix\n",
        "\n",
        "avg_conf_matrix = compute_confusion_matrix(alexnet_model, dataset, num_runs)\n",
        "print(f\"Average Confusion Matrix : \\n\\n {avg_conf_matrix}\")\n",
        "\n",
        "# Mengubah Confusi Matrix menjadi Integers\n",
        "avg_conf_matrix = avg_conf_matrix.astype(int)\n",
        "\n",
        "class_names = class_names\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a056fd8d-7cdd-4843-a342-b1bdfae82c81",
      "metadata": {
        "id": "a056fd8d-7cdd-4843-a342-b1bdfae82c81"
      },
      "source": [
        "### 10. Membuat HeatMap Data Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "405d3056-c0f9-47f1-8169-10b43a07f974",
      "metadata": {
        "id": "405d3056-c0f9-47f1-8169-10b43a07f974"
      },
      "outputs": [],
      "source": [
        "# Membuat Plot Heat Map\n",
        "\n",
        "plt.figure(figsize = (10,8))\n",
        "sns.heatmap(avg_conf_matrix, annot=True, fmt='d', cmap = 'Greens', cbar = True,\n",
        "           xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22581c8-2def-40f4-acf4-a13aa7ff65e8",
      "metadata": {
        "id": "f22581c8-2def-40f4-acf4-a13aa7ff65e8"
      },
      "source": [
        "### 11. Menampilkan Konfusi Matriks pada Data Validasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be109f6-ba99-41b6-b563-4aca403780ef",
      "metadata": {
        "id": "0be109f6-ba99-41b6-b563-4aca403780ef"
      },
      "outputs": [],
      "source": [
        "# Menampilkan Confusi Matriks dalam Data Latihan\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Contoh Penggunaan\n",
        "num_runs = 5 # Bilangan running rerata sampai lebih dari -n\n",
        "dataset = val_dataset # dataset tensorflow\n",
        "# model = tf.keras.models.load_model('model_class_13_balances_combined_ep_50.h5')  # Your trained TensorFlow model\n",
        "\n",
        "def compute_confusion_matrix(alexnet_model, dataset, num_runs):\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "\n",
        "        # Membuat Prediksi dalam dataset\n",
        "        for features, labels in dataset:\n",
        "            predictions = alexnet_model.predict(features)\n",
        "            true_labels.extend(labels.numpy())\n",
        "            pred_labels.extend(np.argmax(predictions, axis = 1))\n",
        "\n",
        "        all_true_labels.append(true_labels)\n",
        "        all_pred_labels.append(pred_labels)\n",
        "\n",
        "    #Mengkomputasi Confusi Matrix untuk tiap Running\n",
        "    all_conf_matrices = [confusion_matrix(true_labels, pred_labels) for true_labels, pred_labels in zip(all_true_labels, all_pred_labels)]\n",
        "\n",
        "    # rerata Confusi Matrix\n",
        "    avg_conf_matrix = np.mean(all_conf_matrices, axis=0)\n",
        "\n",
        "    return avg_conf_matrix\n",
        "\n",
        "avg_conf_matrix = compute_confusion_matrix(alexnet_model, dataset, num_runs)\n",
        "print(f\"Average Confusion Matrix : \\n\\n {avg_conf_matrix}\")\n",
        "\n",
        "# Mengubah Confusi Matrix menjadi Integers\n",
        "avg_conf_matrix = avg_conf_matrix.astype(int)\n",
        "\n",
        "class_names = class_names\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241cbb7e-a27d-4fbc-acdd-b90c24816fc1",
      "metadata": {
        "id": "241cbb7e-a27d-4fbc-acdd-b90c24816fc1"
      },
      "source": [
        "### 12. Membuat HeatMap Data Validasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabb5389-298c-4861-b9e7-8f118a96930c",
      "metadata": {
        "id": "eabb5389-298c-4861-b9e7-8f118a96930c",
        "outputId": "100e9b85-48bf-420c-c0c7-99a1d9384795"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Membuat Plot Heat Map\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      4\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(avg_conf_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreens\u001b[39m\u001b[38;5;124m'\u001b[39m, cbar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m            xticklabels\u001b[38;5;241m=\u001b[39mclass_names, yticklabels\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Labels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# Membuat Plot Heat Map\n",
        "\n",
        "plt.figure(figsize = (10,8))\n",
        "sns.heatmap(avg_conf_matrix, annot=True, fmt='d', cmap = 'Greens', cbar = True,\n",
        "           xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21fbcca9-fdd8-406e-aaad-b5adda3660fe",
      "metadata": {
        "id": "21fbcca9-fdd8-406e-aaad-b5adda3660fe"
      },
      "source": [
        "## 13. Menampilkan Laporan Klasifikasi Data Training dan Validasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684915d9-c0a0-4619-8e67-39c10694e764",
      "metadata": {
        "id": "684915d9-c0a0-4619-8e67-39c10694e764"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Contoh Penggunaan\n",
        "num_runs = 5\n",
        "dataset = train_dataset\n",
        "\n",
        "\n",
        "def compute_classification_report(model, dataset):\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    # membuat prediksi pada dataset\n",
        "    for features, labels in dataset:\n",
        "        predictions = alexnet_model.predict(features)\n",
        "        true_labels.extend(labels.numpy())\n",
        "        pred_labels.extend(np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Mengkomputasikan laporan Klasifikasi\n",
        "    classification_report_str = classification_report(true_labels, pred_labels)\n",
        "    return classification_report_str\n",
        "\n",
        "\n",
        "# Mengkomputasi Hasil Laporan Klasifikasi\n",
        "classification_report_result = compute_classification_report(alexnet_model, dataset)\n",
        "\n",
        "# Menampilkan Laporan Klasifikasi\n",
        "print(f\"Classification Report :\\n\\n {classification_report_result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f36068-612b-4f3c-b0be-b6b715411331",
      "metadata": {
        "id": "40f36068-612b-4f3c-b0be-b6b715411331"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Contoh Penggunaan\n",
        "num_runs = 5\n",
        "dataset = val_dataset\n",
        "\n",
        "\n",
        "def compute_classification_report(model, dataset):\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    # membuat prediksi pada dataset\n",
        "    for features, labels in dataset:\n",
        "        predictions = alexnet_model.predict(features)\n",
        "        true_labels.extend(labels.numpy())\n",
        "        pred_labels.extend(np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Mengkomputasikan laporan Klasifikasi\n",
        "    classification_report_str = classification_report(true_labels, pred_labels)\n",
        "    return classification_report_str\n",
        "\n",
        "\n",
        "# Mengkomputasi Hasil Laporan Klasifikasi\n",
        "classification_report_result = compute_classification_report(alexnet_model, dataset)\n",
        "\n",
        "# Menampilkan Laporan Klasifikasi\n",
        "print(f\"Classification Report :\\n\\n {classification_report_result}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}