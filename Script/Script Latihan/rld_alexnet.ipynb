{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54993b70-da49-4ad2-8883-9052065af4d2",
   "metadata": {},
   "source": [
    "# Klasifikasi Citra Sederhana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd3b28-cd57-47a4-ac8a-1fde973ae7aa",
   "metadata": {},
   "source": [
    "## 1. Mengimport Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc206d1-1bdb-4674-a132-35cd6840ba3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'set_backend' from 'keras.src.utils.backend_utils' (C:\\Users\\umalb\\.conda\\envs\\percobaan\\Lib\\site-packages\\keras\\src\\utils\\backend_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlexNet\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalAveragePooling2D, Dense\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\.conda\\envs\\percobaan\\Lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n",
      "File \u001b[1;32m~\\.conda\\envs\\percobaan\\Lib\\site-packages\\keras\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m # Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\u001b[0;32m      2\u001b[0m #\n\u001b[0;32m      3\u001b[0m # Licensed under the Apache License, Version 2.0 (the \"License\");\n\u001b[0;32m      4\u001b[0m # you may not use this file except in compliance with the License.\n\u001b[0;32m      5\u001b[0m # You may obtain a copy of the License at\n\u001b[0;32m      6\u001b[0m #\n\u001b[0;32m      7\u001b[0m #     http://www.apache.org/licenses/LICENSE-2.0\n\u001b[1;32m----> 8\u001b[0m #\n\u001b[0;32m      9\u001b[0m # Unless required by applicable law or agreed to in writing, software\n\u001b[0;32m     10\u001b[0m # distributed under the License is distributed on an \"AS IS\" BASIS,\n\u001b[0;32m     11\u001b[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\u001b[0;32m     12\u001b[0m # See the License for the specific language governing permissions and\n\u001b[0;32m     13\u001b[0m # limitations under the License.\n\u001b[0;32m     14\u001b[0m # ==============================================================================\n\u001b[0;32m     15\u001b[0m \"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m Detailed documentation and user guides are available at\n\u001b[0;32m     18\u001b[0m [keras.io](https://keras.io).\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m # pylint: disable=unused-import\n",
      "File \u001b[1;32m~\\.conda\\envs\\percobaan\\Lib\\site-packages\\keras\\_tf_keras\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[1;32m~\\.conda\\envs\\percobaan\\Lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constraints\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n",
      "File \u001b[1;32m~\\.conda\\envs\\percobaan\\Lib\\site-packages\\keras\\config\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype_policies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_dtype_policy\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_unsafe_deserialization\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_backend\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_interactive_logging\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_interactive_logging\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'set_backend' from 'keras.src.utils.backend_utils' (C:\\Users\\umalb\\.conda\\envs\\percobaan\\Lib\\site-packages\\keras\\src\\utils\\backend_utils.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684306bb-7792-4816-a6df-26a0e6c05795",
   "metadata": {},
   "source": [
    "## 2. Menginput Gambar dan Membuatnya dalam bentuk Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5435ee7-2f21-4664-8ce9-8fe34e8bebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Penggunaan\n",
    "image_dir = 'D:\\Perkuliahan\\Mata Kuliah\\MBKM\\BRIN\\Data\\data_kaggle\\rice_leaf_diseases' #address folder gambar untuk dataset\n",
    "iamge_size = (224,224) #input ukuran pada gambar\n",
    "batch_size = 32 # ukuran batch awal sebesar 32\n",
    "\n",
    "#Membuat Dataset Latihan (80% dari data total)\n",
    "train_dataset = tf.keras.src.utils.image_dataset_from_directory(\n",
    "    directory = image_dir, #variabel pengarahan address dataset\n",
    "    validation_split = 0.2, #20% data ini digunakan untuk validasi\n",
    "    subset = 'training',\n",
    "    seed = 123, #Random Seed untuk pergantian (shuffling)\n",
    "    image_size = image_size, # untuk menspesifikasi ukuran gambar pd variabel image_size\n",
    "    batch)size = batch_size\n",
    ")\n",
    "\n",
    "#Membuat Dataset Validasi (20% dari data total)\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = image_dir,\n",
    "    validation_split = 0.2, # 20% data tersebut untuk validasi\n",
    "    subset = 'validation',\n",
    "    seed = 123, # Random Seed untuk Pergantian (Data acak)\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size\n",
    ") # sebagai variabel input untuk mendapatkan nilai akurasi, loss, dll pada validasi\n",
    "\n",
    "class_name = train_dataset.class_names\n",
    "#print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc75b87-4234-4971-aa90-37b0cc92a06d",
   "metadata": {},
   "source": [
    "## 3. Menampilkan Dataset awal dari Sampel Acak sebanyak 20 Gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63939d2a-4939-40e6-b180-7a2a8e00f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan 4x4 grid pada Gambar Latihan dari Dataset Tensorflow menggunakan tf.keras.utils.image_dataset_from_directory\n",
    "figur = plt.figure(figsize = (13,13))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range (20): #menampilkan 20 sampel random gambar latihan\n",
    "        ax = plt.subplot(5,4, i+1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee31fe-d61e-48b4-9654-fc54c7686235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan 4x4 grid pada Gambar Validasi dari Dataset Tensorflow menggunakan tf.keras.utils.image_dataset_from_directory\n",
    "figur = plt.figure(figsize = (13,13))\n",
    "for images, labels in val_dataset.take(1):\n",
    "    for i in range (20): #menampilkan 20 sampel random gambar latihan\n",
    "        ax = plt.subplot(5,4, i+1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fe74d-eba5-474f-a197-58e8c5a4912a",
   "metadata": {},
   "source": [
    "## 4. Melakukan Ekstraksi Fitur Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d03c8-ae66-48a5-be8c-bd57ad2c9493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langkah 1 : Membuat Dataset Tensorflow dari Images Directory\n",
    "def create_dataset(image_dir, image_size, batch_size):\n",
    "    #Memuat dan memproses gambar dari Directory tersebut\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "    # Menggunakan ImageDataGenerator untuk memuat dan memproses gambar dari direktori.\n",
    "    # Rescale 1./255 digunakan untuk mengubah rentang piksel menjadi antara 0 dan 1.\n",
    "    dataset = datagen.flow_from_directory(\n",
    "        directory=image_dir,\n",
    "        target_size = image_size, # ukuran target gambar yang diinginkan\n",
    "        batch_size = batch_size, # ukuran batch data yang akan dimuat pada setiap iterasi\n",
    "        class_mode = None, # Mode kelas. None berarti kita tidak membutuhkan label kelas karena kita menggunakan dataset ini untuk prediksi, bukan klasifikasi.\n",
    "        shuffle = False # shuffle=False berarti kita ingin menjaga urutan gambar sesuai dengan yang ada di direktori.\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "# Langkah 2 : mendefinisikan AlexNet Model untuk Ekstraksi Fitur\n",
    "def create_alexnet_model(input_shape, num_classes):\n",
    "    # Buat Input Layer\n",
    "    input_layer = Input(shape = input_shape)\n",
    "    #Conv Layer 1\n",
    "    x = Conv2D(filters=96, kernel_size=(11,11), strides = (4,4), activation = 'relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size = (3,3), strides = (2,2))(x)\n",
    "    #Conv Layer 2\n",
    "    x = Conv2D(filters=256, kernel_size=(5,5), padding='same', activation = 'relu')\n",
    "    x = MaxPooling2D(pool_size = (3,3), strides = (2,2))(x)\n",
    "    #Conv Layer 3\n",
    "    x = Conv2D(filters=384, kernel_size=(3,3), padding='same', activation = 'relu')\n",
    "    #Conv Layer 4\n",
    "    x = Conv2D(filters=384, kernel_size=(3,3), padding='same', activation = 'relu')\n",
    "    #Conv Layer 5\n",
    "    x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation = 'relu')\n",
    "    x = MaxPooling2D(pool_size = (3,3), strides = (2,2))(x)\n",
    "    #Flatten Layer\n",
    "    x = Flatten()(x)\n",
    "    #Fully Connected Layer 1\n",
    "    x = Dense(4096, activation = 'relu')(x)\n",
    "    #Fully Connected Layer 2\n",
    "    x = Dense(4096, activation = 'relu')(x)\n",
    "    #Output Layer\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    #Buat Model dari input layer dan output layer\n",
    "    model = tf.keras.Model(inputs=input_layer, output=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Langkah 3 : Mendeteksi Penyakit Daun Padi dan Menyediakan Bounding Boxes\n",
    "def detect_and_bbox(dataset, model):\n",
    "    predictions = model.predict(dataset, verbose =1)\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b5e16-e294-4142-860b-20143ce2bc28",
   "metadata": {},
   "source": [
    "## 5. Mendefinisikan AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "768c15bb-c604-41a5-8fc0-cd7935a050a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_shape(\u001b[38;5;241m*\u001b[39mimage_size, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m#bentuk daripada input gambar\u001b[39;00m\n\u001b[0;32m      2\u001b[0m alexnet_model \u001b[38;5;241m=\u001b[39m create_alexnet_model(input_shape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape(*image_size, 3) #bentuk daripada input gambar\n",
    "num_classes = class_name # menentukan kelas dataset\n",
    "# Buat model AlexNet\n",
    "alexnet_model = create_alexnet_model(input_shape, num_classes)\n",
    "# menampilkan ringkasan model\n",
    "alexnet_model.summary() #digunakan untuk memeriksa arsitektur yang kita buat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569b275-5f89-4387-9e7d-e83c332c526f",
   "metadata": {},
   "source": [
    "## 6. Mengcompile AlexNet dengan Pendekatan Aturan untuk TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5baa48-bde2-4cbe-85fd-dc3565d4782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyusun Model AlexNet dengan Pendekatan untuk Transfer Learning\n",
    "alexnet_model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics = ['accuracy']) # Loss = 'mean_squared_error', metrics = ['mse']\n",
    "\n",
    "num_epochs = 20\n",
    "# Melatih model tersebut dan mendapatkan jejak latihan\n",
    "# alexnet_model.fit(train_dataset, epoch=num_epochs, validation_data = val_dataset)\n",
    "history = alexnet_model.fit(train_dataset, epoch=num_epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a64fb-32f7-43f1-86e7-bcef2c506d63",
   "metadata": {},
   "source": [
    "## 7. Membuat Plot untuk Loss pada Latihan dan Validasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20639535-c192-4edc-bf36-61d6fb541493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Plot Latihan Vs Plot Validasi\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Data Accuracy')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Data Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b85ed5-a11b-48fc-a99a-88a7ab1bb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan nilai akurasi dan loss pada nilai set validasi\n",
    "loss, accuracy = alexnet_model.evaluate(val_dataset)\n",
    "print(f\"Validation Loss {loss}\")\n",
    "print(f\"Validation Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8bec5-042f-4bb0-9866-c8c92f16de22",
   "metadata": {},
   "source": [
    "## 8. Mendeteksi Penyakit Daun Padi dan Menyediakan Kotak Pembatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d78f1c9-0f79-4fc1-8226-397448fb392c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_and_bbox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Mendeteksi dan membuat Kotak Pembatas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m detect_and_bbox(val_dataset, alexnet_model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detect_and_bbox' is not defined"
     ]
    }
   ],
   "source": [
    "# Mendeteksi dan membuat Kotak Pembatas\n",
    "predictions = detect_and_bbox(val_dataset, alexnet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719aefe-4ecd-4928-9edd-39c14a097798",
   "metadata": {},
   "source": [
    "## 9. Menampilkan Konfusi Matriks Data Latihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf332984-513f-4634-8a8a-69fcb804e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan Confusi Matriks dalam Data Latihan\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Contoh Penggunaan\n",
    "num_runs = 5 # Bilangan running rerata sampai lebih dari -n\n",
    "dataset = train_dataset # dataset tensorflow\n",
    "# model = tf.keras.models.load_model('model_class_13_balances_combined_ep_50.h5')  # Your trained TensorFlow model\n",
    "\n",
    "def compute_confusion_matrix(alexnet_model, dataset, num_runs):\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        # Membuat Prediksi dalam dataset\n",
    "        for features, labels in dataset:\n",
    "            predictions = alexnet_model.predict(features)\n",
    "            true_labels.extend(labels.numpy())\n",
    "            pred_labels.extend(np.argmax(predictions, axis = 1))\n",
    "\n",
    "        all_true_labels.append(true_labels)\n",
    "        all_pred_labels.append(pred_labels)\n",
    "\n",
    "    #Mengkomputasi Confusi Matrix untuk tiap Running\n",
    "    all_conf_matrices = [confusion_matrix(true_labels, pred_labels) for true_labels, pred_labels in zip(all_true_labels, all_pred_labels)]\n",
    "\n",
    "    # rerata Confusi Matrix\n",
    "    avg_conf_matrix = np.mean(all_conf_matrices, axis=0)\n",
    "\n",
    "    return avg_conf_matrix\n",
    "\n",
    "avg_conf_matrix = compute_confusion_matrix(alexnet_model, dataset, num_runs)\n",
    "print(f\"Average Confusion Matrix : \\n\\n {avg_conf_matrix}\")\n",
    "\n",
    "# Mengubah Confusi Matrix menjadi Integers\n",
    "avg_conf_matrix = avg_conf_matrix.astype(int)\n",
    "\n",
    "class_names = class_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056fd8d-7cdd-4843-a342-b1bdfae82c81",
   "metadata": {},
   "source": [
    "## 10. Membuat HeatMap Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d3056-c0f9-47f1-8169-10b43a07f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Plot Heat Map\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.heatmap(avg_conf_matrix, annot=True, fmt='d', cmap = 'Greens', cbar = True,\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22581c8-2def-40f4-acf4-a13aa7ff65e8",
   "metadata": {},
   "source": [
    "## 11. Menampilkan Konfusi Matriks pada Data Validasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be109f6-ba99-41b6-b563-4aca403780ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan Confusi Matriks dalam Data Latihan\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Contoh Penggunaan\n",
    "num_runs = 5 # Bilangan running rerata sampai lebih dari -n\n",
    "dataset = val_dataset # dataset tensorflow\n",
    "# model = tf.keras.models.load_model('model_class_13_balances_combined_ep_50.h5')  # Your trained TensorFlow model\n",
    "\n",
    "def compute_confusion_matrix(alexnet_model, dataset, num_runs):\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        # Membuat Prediksi dalam dataset\n",
    "        for features, labels in dataset:\n",
    "            predictions = alexnet_model.predict(features)\n",
    "            true_labels.extend(labels.numpy())\n",
    "            pred_labels.extend(np.argmax(predictions, axis = 1))\n",
    "\n",
    "        all_true_labels.append(true_labels)\n",
    "        all_pred_labels.append(pred_labels)\n",
    "\n",
    "    #Mengkomputasi Confusi Matrix untuk tiap Running\n",
    "    all_conf_matrices = [confusion_matrix(true_labels, pred_labels) for true_labels, pred_labels in zip(all_true_labels, all_pred_labels)]\n",
    "\n",
    "    # rerata Confusi Matrix\n",
    "    avg_conf_matrix = np.mean(all_conf_matrices, axis=0)\n",
    "\n",
    "    return avg_conf_matrix\n",
    "\n",
    "avg_conf_matrix = compute_confusion_matrix(alexnet_model, dataset, num_runs)\n",
    "print(f\"Average Confusion Matrix : \\n\\n {avg_conf_matrix}\")\n",
    "\n",
    "# Mengubah Confusi Matrix menjadi Integers\n",
    "avg_conf_matrix = avg_conf_matrix.astype(int)\n",
    "\n",
    "class_names = class_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cbb7e-a27d-4fbc-acdd-b90c24816fc1",
   "metadata": {},
   "source": [
    "## 12. Membuat HeatMap Data Validasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabb5389-298c-4861-b9e7-8f118a96930c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Membuat Plot Heat Map\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      4\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(avg_conf_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreens\u001b[39m\u001b[38;5;124m'\u001b[39m, cbar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m            xticklabels\u001b[38;5;241m=\u001b[39mclass_names, yticklabels\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Labels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Membuat Plot Heat Map\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.heatmap(avg_conf_matrix, annot=True, fmt='d', cmap = 'Greens', cbar = True,\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbcca9-fdd8-406e-aaad-b5adda3660fe",
   "metadata": {},
   "source": [
    "## 13. Menampilkan Laporan Klasifikasi Data Training dan Validasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684915d9-c0a0-4619-8e67-39c10694e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Contoh Penggunaan\n",
    "num_runs = 5\n",
    "dataset = train_dataset\n",
    "\n",
    "\n",
    "def compute_classification_report(model, dataset):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    # membuat prediksi pada dataset\n",
    "    for features, labels in dataset:\n",
    "        predictions = alexnet_model.predict(features)\n",
    "        true_labels.extend(labels.numpy())\n",
    "        pred_labels.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "    # Mengkomputasikan laporan Klasifikasi\n",
    "    classification_report_str = classification_report(true_labels, pred_labels)\n",
    "    return classification_report_str\n",
    "\n",
    "\n",
    "# Mengkomputasi Hasil Laporan Klasifikasi\n",
    "classification_report_result = compute_classification_report(alexnet_model, dataset)\n",
    "\n",
    "# Menampilkan Laporan Klasifikasi \n",
    "print(f\"Classification Report :\\n\\n {classification_report_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f36068-612b-4f3c-b0be-b6b715411331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Contoh Penggunaan\n",
    "num_runs = 5\n",
    "dataset = val_dataset\n",
    "\n",
    "\n",
    "def compute_classification_report(model, dataset):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    # membuat prediksi pada dataset\n",
    "    for features, labels in dataset:\n",
    "        predictions = alexnet_model.predict(features)\n",
    "        true_labels.extend(labels.numpy())\n",
    "        pred_labels.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "    # Mengkomputasikan laporan Klasifikasi\n",
    "    classification_report_str = classification_report(true_labels, pred_labels)\n",
    "    return classification_report_str\n",
    "\n",
    "\n",
    "# Mengkomputasi Hasil Laporan Klasifikasi\n",
    "classification_report_result = compute_classification_report(alexnet_model, dataset)\n",
    "\n",
    "# Menampilkan Laporan Klasifikasi \n",
    "print(f\"Classification Report :\\n\\n {classification_report_result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
